{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMhZ8PqSeZ2xWn5Xlup8Wu9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonseong0917/NLP_study/blob/main/Korean_Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0JfQzPWSW3A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "metadata": {
        "id": "kJkpz3hxS2_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "dataset = dataset.data"
      ],
      "metadata": {
        "id": "EC3WmCdIS4u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "oSQr3rcjTBCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"데이터셋 총 개수 : {len(dataset)}\")"
      ],
      "metadata": {
        "id": "Pf5_KE4-TE2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = pd.DataFrame({'document':dataset})\n",
        "news_df"
      ],
      "metadata": {
        "id": "T9dNWK_WTHGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 먼저 데이터셋 내에 결측값이 있는지 확인합니다\n",
        "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
        "print(news_df.isnull().values.any())"
      ],
      "metadata": {
        "id": "VyseVyuDTejN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = news_df.dropna().reset_index(drop=True)\n",
        "print(f\"필터링된 데이터셋 총 개수 : {len(news_df)}\")"
      ],
      "metadata": {
        "id": "PkmxtbNFTlws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df"
      ],
      "metadata": {
        "id": "iPC_P0WxTvSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 열을 기준으로 중복제거\n",
        "processed_news_df = news_df.drop_duplicates(['document']).reset_index(drop=True)\n",
        "processed_news_df"
      ],
      "metadata": {
        "id": "gH4DfVWmTxPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(processed_news_df.iloc[0][0])"
      ],
      "metadata": {
        "id": "ziX5tIRWT8eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_news_df.iloc[0][0]"
      ],
      "metadata": {
        "id": "cs9463vAT_T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 내 특수 문자를 제거합니다.\n",
        "processed_news_df['document'] = processed_news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "processed_news_df"
      ],
      "metadata": {
        "id": "T6loMIjZUC8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 내 길이가 너무 짧은 단어를 제거합니다. (단어의 길이가 2 이하)\n",
        "processed_news_df['document'] = processed_news_df['document'].apply(lambda x: ' '.join([token for token in x.split() if len(token) > 2]))\n",
        "processed_news_df"
      ],
      "metadata": {
        "id": "4dqYa3xlUKCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 길이가 200 이하이거나 전체 단어 개수가 5개 이하인 데이터를 필터링합니다.\n",
        "processed_news_df = processed_news_df[processed_news_df.document.apply(lambda x: len(str(x)) <= 200 and len(str(x).split()) > 5)].reset_index(drop=True)\n",
        "processed_news_df"
      ],
      "metadata": {
        "id": "o3nFbpPzUQGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 단어에 대한 소문자 변환 (정규화)\n",
        "processed_news_df['document'] = processed_news_df['document'].apply(lambda x: x.lower())\n",
        "processed_news_df"
      ],
      "metadata": {
        "id": "5h6l-FWyUsmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "iclGpSkBUulF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "NxsQ9r1WUwNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "print(len(stop_words))\n",
        "print(stop_words[:10])"
      ],
      "metadata": {
        "id": "-rJgWvauVVsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터들의 불용어를 제외하여 띄어쓰기 단위로 문장을 분리합니다.\n",
        "tokenized_doc = processed_news_df['document'].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [s_word for s_word in x if s_word not in stop_words])\n",
        "tokenized_doc"
      ],
      "metadata": {
        "id": "yx2hOrhyVY9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_doc = tokenized_doc.to_list()\n",
        "print(len(tokenized_doc))"
      ],
      "metadata": {
        "id": "R-4Mzkw1cIYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"최종 학습 데이터셋 수 : {len(tokenized_doc)}\")"
      ],
      "metadata": {
        "id": "w46Cwq3odXIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "hQWudltndwbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_doc)"
      ],
      "metadata": {
        "id": "X8nE_9bfdzFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = tokenizer.word_index\n",
        "idx2word = {value : key for key, value in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
      ],
      "metadata": {
        "id": "-JgXruVVd09G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2idx) + 1 \n",
        "print(f\"단어 사전의 크기 : {vocab_size}\")"
      ],
      "metadata": {
        "id": "_4odzaXilcri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded[0])"
      ],
      "metadata": {
        "id": "O2krM71_lgsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import skipgrams"
      ],
      "metadata": {
        "id": "L_vuHjZyljlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네거티브 샘플링\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:5]]\n",
        "print(f\"전체 샘플 수 : {len(skip_grams)}\")"
      ],
      "metadata": {
        "id": "hlD3Zd_Dl6kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]"
      ],
      "metadata": {
        "id": "6xvFdC7UmA_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"first 3 pairs: {pairs[:3]}\")\n",
        "print(f\"first 3 labels: {labels[:3]}\")"
      ],
      "metadata": {
        "id": "38jBVM2rmMs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
        "print(len(pairs))\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "8TVmWGNqms79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "    idx2word[pairs[i][0]], pairs[i][0], \n",
        "    idx2word[pairs[i][1]], pairs[i][1], \n",
        "    labels[i])\n",
        "  )"
      ],
      "metadata": {
        "id": "sOBsgW9Dm27R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:1000]]\n",
        "print(training_dataset[0][0])"
      ],
      "metadata": {
        "id": "iUNHpRD7nSWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(training_dataset))"
      ],
      "metadata": {
        "id": "nSM410xInY23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG"
      ],
      "metadata": {
        "id": "ataVYNeOnaUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "# 중심 단어를 위한 임베딩 테이블\n",
        "w_inputs = Input(shape=(1, ), dtype='int32')\n",
        "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n",
        "\n",
        "# 주변 단어를 위한 임베딩 테이블\n",
        "c_inputs = Input(shape=(1, ), dtype='int32')\n",
        "context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)"
      ],
      "metadata": {
        "id": "okcJ7wn8ndGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "print(dot_product)"
      ],
      "metadata": {
        "id": "c6pl476mnxff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "print(dot_product)\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "print(dot_product)\n",
        "output = Activation('sigmoid')(dot_product)\n",
        "# print(output[0][0])\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# tf.config.run_functions_eagerly(True)\n",
        "# output.numpy()\n",
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "print(model.output)\n",
        "# model.summary()\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "DC1KKMj-W44F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "output = Activation('sigmoid')(dot_product)\n",
        "\n",
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
      ],
      "metadata": {
        "id": "BPRqztUmn0tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  loss = 0\n",
        "  for _, elem in enumerate(skip_grams):\n",
        "    first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "    second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "    labels = np.array(elem[1], dtype='int32')\n",
        "    X = [first_elem, second_elem]\n",
        "    Y = labels\n",
        "    loss += model.train_on_batch(X,Y)  \n",
        "  print('Epoch :',epoch + 1, 'Loss :',loss)"
      ],
      "metadata": {
        "id": "t8Oa_jQLodoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "OtYlySOMo7Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
        "vectors = model.get_weights()[0]\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "5XssbmPR1dFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 로드\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
      ],
      "metadata": {
        "id": "me3gxmN61eUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=w2v.most_similar(negative=['apple'])\n",
        "k\n",
        "# k[0]"
      ],
      "metadata": {
        "id": "cOv5MTe51jie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "texts=[]\n",
        "for i in range(len(data)):\n",
        "  texts.append(data[i][\"텍스트\"].replace(\"\\n\",\"\"))\n",
        "print(texts)  \n"
      ],
      "metadata": {
        "id": "ha3GVviW1ukT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "file_path=\"./readability_subject_data.json\"\n",
        "k=[]\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "    k=data[:]\n",
        "texts=[]\n",
        "for i in range(len(k)):\n",
        "  texts.append(k[i][\"텍스트\"].replace(\"\\n\",\"\"))"
      ],
      "metadata": {
        "id": "HRlSqLXnGMLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kor_tokenized_doc = [text.split() for text in texts]\n",
        "print(len(kor_tokenized_doc))"
      ],
      "metadata": {
        "id": "V_AtcgrMLFDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "jFwrG58XLQSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(kor_tokenized_doc)"
      ],
      "metadata": {
        "id": "3YDfMMRyLRqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = tokenizer.word_index\n",
        "idx2word = {value : key for key, value in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(kor_tokenized_doc)"
      ],
      "metadata": {
        "id": "gOHiWK_KLTCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2idx) + 1 \n",
        "print(f\"단어 사전의 크기 : {vocab_size}\")"
      ],
      "metadata": {
        "id": "6bWcWIATLWc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded[0])"
      ],
      "metadata": {
        "id": "MesJaVDRLYvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import skipgrams"
      ],
      "metadata": {
        "id": "spHNFopqLc6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네거티브 샘플링\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:5]]\n",
        "print(f\"전체 샘플 수 : {len(skip_grams)}\")"
      ],
      "metadata": {
        "id": "76XNKRP6Lgvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]"
      ],
      "metadata": {
        "id": "Dd-paOIALjhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
        "print(len(pairs))\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "Lt0QTHrFLmNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs)"
      ],
      "metadata": {
        "id": "cSZrjmlSMBjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "    idx2word[pairs[i][0]], pairs[i][0], \n",
        "    idx2word[pairs[i][1]], pairs[i][1], \n",
        "    labels[i])\n",
        "  )"
      ],
      "metadata": {
        "id": "t4zMiCsvLn-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
      ],
      "metadata": {
        "id": "HMhE0Sg0Lqh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG"
      ],
      "metadata": {
        "id": "QahYAnU8LzY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "# 중심 단어를 위한 임베딩 테이블\n",
        "w_inputs = Input(shape=(1, ), dtype='int32')\n",
        "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n",
        "\n",
        "# 주변 단어를 위한 임베딩 테이블\n",
        "c_inputs = Input(shape=(1, ), dtype='int32')\n",
        "context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)"
      ],
      "metadata": {
        "id": "0ITgG5JXL2uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "output = Activation('sigmoid')(dot_product)\n",
        "\n",
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
      ],
      "metadata": {
        "id": "AM4CAygPL54e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  loss = 0\n",
        "  for _, elem in enumerate(skip_grams):\n",
        "    first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "    second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "    labels = np.array(elem[1], dtype='int32')\n",
        "    X = [first_elem, second_elem]\n",
        "    Y = labels\n",
        "    loss += model.train_on_batch(X,Y)  \n",
        "  print('Epoch :',epoch + 1, 'Loss :',loss)"
      ],
      "metadata": {
        "id": "LZ5GBuoTMdEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('kor_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
        "vectors = model.get_weights()[0]\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "N243nejNMhza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 로드\n",
        "import re\n",
        "import random\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./kor_vectors.txt', binary=False)\n",
        "sep_sentence=[]\n",
        "res=[]\n",
        "res_ans=[]\n",
        "for i in range(len(texts)):\n",
        "  # print(texts[i])\n",
        "  cur_text=texts[i]\n",
        "  # print(cur_text.split(\".\"))\n",
        "  first_split_list=cur_text.split(\".\")\n",
        "  \n",
        "  for j in range(len(first_split_list)):\n",
        "    if first_split_list[j] != '':\n",
        "      sep_sentence.append(first_split_list[j])\n",
        "# print(sep_sentence)\n",
        "# print(len(sep_sentence))      \n",
        "for i in range(len(sep_sentence)):\n",
        "  cur_text=sep_sentence[i] # 문장 하나\n",
        "  res.append(cur_text)\n",
        "  res_ans.append(1)\n",
        "  cur_text_words=cur_text.split(\" \") # 문장을 단어별로 쪼갠 리스트\n",
        "  candidate_numbers=[x for x in range(len(cur_text_words))] # 현재 문장에서 어떤 index의 단어들을 replace할 것인지\n",
        "  random.shuffle(candidate_numbers) # 랜덤으로 선택하기 위해 shuffle\n",
        "  flag=0\n",
        "  if(len(candidate_numbers)>=5):\n",
        "    selected_numbers=candidate_numbers[:5]\n",
        "    for j in range(3):\n",
        "      cur_word=cur_text_words[selected_numbers[j]] # 바꿀 단어\n",
        "      if cur_word in w2v:\n",
        "        flag=1\n",
        "        t=w2v.most_similar(negative=[cur_word])\n",
        "        # print(t)\n",
        "        for k in range(len(t)):\n",
        "          negative_word=t[k][0]\n",
        "          cur_text_words[selected_numbers[j]]=negative_word\n",
        "\n",
        "        target=\"\"\n",
        "        for m in range(len(cur_text_words)):\n",
        "          target+=cur_text_words[m]\n",
        "          if m != len(cur_text_words)-1:\n",
        "            target+=\" \"\n",
        "          else:\n",
        "            target+=\".\"\n",
        "        res.append(target)\n",
        "        res_ans.append(0)\n",
        "print(len(res))        \n",
        "print(len(res_ans))\n",
        "from pandas import DataFrame\n",
        "raw_data={'sentence': res, 'ans': res_ans}\n",
        "data=DataFrame(raw_data)\n",
        "data.sample(n=5)\n",
        "for i in range(len(res)):\n",
        "  print(res[i])\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "0vv0ey-pMl97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "_yKVxbd6_L6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet # 코랩 환경이기 때문에 앞에 !를 붙여야 한다.\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==3.0.2\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "GYKdPaD0MneS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "id": "ndUE_qELMsdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp pandas tqdm\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "JmK_iQuG103l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.0.2"
      ],
      "metadata": {
        "id": "P2-lR-3a41gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh"
      ],
      "metadata": {
        "id": "PMsn2WRj6Hre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.0.2"
      ],
      "metadata": {
        "id": "OcYwhn4V6fv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "2idoH9LH7J2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
      ],
      "metadata": {
        "id": "2CcyT9fK7Un0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "metadata": {
        "id": "XeohRDLb7bdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#kobert\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW # 인공지능 모델의 초기값 지정 함수를 아담으로 지정한다.\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "4rmuiQPt7gzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "2fcevavt7lAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "n_devices = torch.cuda.device_count()\n",
        "print(n_devices)\n",
        "\n",
        "for i in range(n_devices):\n",
        "    print(torch.cuda.get_device_name(i))"
      ],
      "metadata": {
        "id": "bExQFIX-7tp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "metadata": {
        "id": "RPZW0uf97vxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model()"
      ],
      "metadata": {
        "id": "DCvVbgfd9EQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# "
      ],
      "metadata": {
        "id": "ARvP_7yH9G8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 15\n",
        "max_grad_norm = 1\n",
        "log_interval = 100\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "rt9mD6Ld9euE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Xl-X0yfB9lTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_list = []\n",
        "for q, label in zip(data['sentence'], data['ans'])  :\n",
        "    cur_data = []\n",
        "    cur_data.append(q)\n",
        "    cur_data.append(str(label))\n",
        "\n",
        "    data_list.append(cur_data)    "
      ],
      "metadata": {
        "id": "YUajzGnJ_ek4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)"
      ],
      "metadata": {
        "id": "tnPqSkitAPW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset_train))\n",
        "print(len(dataset_test))"
      ],
      "metadata": {
        "id": "Eb4xYSwgAQGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "h4gJqbLtARp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting parameters\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "w0OTGobGAdrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
      ],
      "metadata": {
        "id": "nKf8KbUuAgYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train[0]"
      ],
      "metadata": {
        "id": "iFqZ-jC7AjVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ],
      "metadata": {
        "id": "MrDOIyscAop0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=2,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "                 \n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "    \n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "        \n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "3tosraKUAuNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT 모델 불러오기\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "\n",
        "#optimizer와 schedule 설정\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "    \n",
        "train_dataloader"
      ],
      "metadata": {
        "id": "L5Yx8ZGxA0bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    \n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))"
      ],
      "metadata": {
        "id": "fHMLb9YFA7-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰화\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            if np.argmax(logits) == 0:\n",
        "                test_eval.append(\"잘못된 문장\")\n",
        "            elif np.argmax(logits) == 1:\n",
        "                test_eval.append(\"정확한 문장\")\n",
        "            \n",
        "\n",
        "        print(\">> 입력하신 내용은 \" + test_eval[0] + \"일 가능성이 있습니다.\")"
      ],
      "metadata": {
        "id": "kEZNQ8JMBLkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end = 1\n",
        "while end == 1 :\n",
        "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
        "    if sentence == 0 :\n",
        "        break\n",
        "    predict(sentence)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "lJymHYd2CBMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUh_xMwOC9ny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}