{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN+dTVqVRYXPZG4Wa5QQlIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonseong0917/NLP_study/blob/main/Korean_Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0JfQzPWSW3A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "metadata": {
        "id": "kJkpz3hxS2_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "dataset = dataset.data"
      ],
      "metadata": {
        "id": "EC3WmCdIS4u0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "oSQr3rcjTBCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"데이터셋 총 개수 : {len(dataset)}\")"
      ],
      "metadata": {
        "id": "Pf5_KE4-TE2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = pd.DataFrame({'document':dataset})\n",
        "news_df"
      ],
      "metadata": {
        "id": "T9dNWK_WTHGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 먼저 데이터셋 내에 결측값이 있는지 확인합니다\n",
        "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
        "print(news_df.isnull().values.any())"
      ],
      "metadata": {
        "id": "VyseVyuDTejN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = news_df.dropna().reset_index(drop=True)\n",
        "print(f\"필터링된 데이터셋 총 개수 : {len(news_df)}\")"
      ],
      "metadata": {
        "id": "PkmxtbNFTlws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df"
      ],
      "metadata": {
        "id": "iPC_P0WxTvSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 열을 기준으로 중복제거\n",
        "processed_news_df = news_df.drop_duplicates(['document']).reset_index(drop=True)\n",
        "processed_news_df"
      ],
      "metadata": {
        "id": "gH4DfVWmTxPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(processed_news_df.iloc[0][0])"
      ],
      "metadata": {
        "id": "ziX5tIRWT8eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_news_df.iloc[0][0]"
      ],
      "metadata": {
        "id": "cs9463vAT_T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 내 특수 문자를 제거합니다.\n",
        "processed_news_df['document'] = processed_news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "processed_news_df"
      ],
      "metadata": {
        "id": "T6loMIjZUC8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서 내 길이가 너무 짧은 단어를 제거합니다. (단어의 길이가 2 이하)\n",
        "processed_news_df['document'] = processed_news_df['document'].apply(lambda x: ' '.join([token for token in x.split() if len(token) > 2]))\n",
        "processed_news_df"
      ],
      "metadata": {
        "id": "4dqYa3xlUKCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 길이가 200 이하이거나 전체 단어 개수가 5개 이하인 데이터를 필터링합니다.\n",
        "processed_news_df = processed_news_df[processed_news_df.document.apply(lambda x: len(str(x)) <= 200 and len(str(x).split()) > 5)].reset_index(drop=True)\n",
        "processed_news_df"
      ],
      "metadata": {
        "id": "o3nFbpPzUQGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 단어에 대한 소문자 변환 (정규화)\n",
        "processed_news_df['document'] = processed_news_df['document'].apply(lambda x: x.lower())\n",
        "processed_news_df"
      ],
      "metadata": {
        "id": "5h6l-FWyUsmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "iclGpSkBUulF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "NxsQ9r1WUwNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "print(len(stop_words))\n",
        "print(stop_words[:10])"
      ],
      "metadata": {
        "id": "-rJgWvauVVsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터들의 불용어를 제외하여 띄어쓰기 단위로 문장을 분리합니다.\n",
        "tokenized_doc = processed_news_df['document'].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [s_word for s_word in x if s_word not in stop_words])\n",
        "tokenized_doc"
      ],
      "metadata": {
        "id": "yx2hOrhyVY9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_doc = tokenized_doc.to_list()\n",
        "print(len(tokenized_doc))"
      ],
      "metadata": {
        "id": "R-4Mzkw1cIYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"최종 학습 데이터셋 수 : {len(tokenized_doc)}\")"
      ],
      "metadata": {
        "id": "w46Cwq3odXIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "hQWudltndwbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_doc)"
      ],
      "metadata": {
        "id": "X8nE_9bfdzFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = tokenizer.word_index\n",
        "idx2word = {value : key for key, value in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
      ],
      "metadata": {
        "id": "-JgXruVVd09G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2idx) + 1 \n",
        "print(f\"단어 사전의 크기 : {vocab_size}\")"
      ],
      "metadata": {
        "id": "_4odzaXilcri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded[0])"
      ],
      "metadata": {
        "id": "O2krM71_lgsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import skipgrams"
      ],
      "metadata": {
        "id": "L_vuHjZyljlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네거티브 샘플링\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:5]]\n",
        "print(f\"전체 샘플 수 : {len(skip_grams)}\")"
      ],
      "metadata": {
        "id": "hlD3Zd_Dl6kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]"
      ],
      "metadata": {
        "id": "6xvFdC7UmA_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"first 3 pairs: {pairs[:3]}\")\n",
        "print(f\"first 3 labels: {labels[:3]}\")"
      ],
      "metadata": {
        "id": "38jBVM2rmMs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
        "print(len(pairs))\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "8TVmWGNqms79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "    idx2word[pairs[i][0]], pairs[i][0], \n",
        "    idx2word[pairs[i][1]], pairs[i][1], \n",
        "    labels[i])\n",
        "  )"
      ],
      "metadata": {
        "id": "sOBsgW9Dm27R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:1000]]\n",
        "print(training_dataset[0][0])"
      ],
      "metadata": {
        "id": "iUNHpRD7nSWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(training_dataset))"
      ],
      "metadata": {
        "id": "nSM410xInY23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG"
      ],
      "metadata": {
        "id": "ataVYNeOnaUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "# 중심 단어를 위한 임베딩 테이블\n",
        "w_inputs = Input(shape=(1, ), dtype='int32')\n",
        "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n",
        "\n",
        "# 주변 단어를 위한 임베딩 테이블\n",
        "c_inputs = Input(shape=(1, ), dtype='int32')\n",
        "context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)"
      ],
      "metadata": {
        "id": "okcJ7wn8ndGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "print(dot_product)"
      ],
      "metadata": {
        "id": "c6pl476mnxff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "print(dot_product)\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "print(dot_product)\n",
        "output = Activation('sigmoid')(dot_product)\n",
        "# print(output[0][0])\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# tf.config.run_functions_eagerly(True)\n",
        "# output.numpy()\n",
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "print(model.output)\n",
        "# model.summary()\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "DC1KKMj-W44F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "output = Activation('sigmoid')(dot_product)\n",
        "\n",
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
      ],
      "metadata": {
        "id": "BPRqztUmn0tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  loss = 0\n",
        "  for _, elem in enumerate(skip_grams):\n",
        "    first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "    second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "    labels = np.array(elem[1], dtype='int32')\n",
        "    X = [first_elem, second_elem]\n",
        "    Y = labels\n",
        "    loss += model.train_on_batch(X,Y)  \n",
        "  print('Epoch :',epoch + 1, 'Loss :',loss)"
      ],
      "metadata": {
        "id": "t8Oa_jQLodoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim"
      ],
      "metadata": {
        "id": "OtYlySOMo7Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
        "vectors = model.get_weights()[0]\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "5XssbmPR1dFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 로드\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
      ],
      "metadata": {
        "id": "me3gxmN61eUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=w2v.most_similar(negative=['apple'])\n",
        "k\n",
        "# k[0]"
      ],
      "metadata": {
        "id": "cOv5MTe51jie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=[\n",
        "    {\n",
        "        \"교과목\": \"사회\",\n",
        "        \"텍스트\": \"자정 능력: 미생물이 오염 물질을 분해하여 원래의 깨끗한 상태로 되돌리는 능력이다.\"\n",
        "    },\n",
        "    {\n",
        "        \"교과목\": \"사회\",\n",
        "        \"텍스트\": \"남획으로 줄어드는 어족 자원: 참치는 현대식 어업을 통해 대량으로 잡히는 생선 중 하나이다.\\n선망 어선은 길이 2km, 깊이 200m정도인 대형 그물을 이용하여 참치뿐만 아니라 멸종 위기에 처한 상어, 가오리, 고래, 바다거북 등을 가리지 않고 잡아들인다.\\n이른바 '혼획'이라고 하는 이 어업 방식으로 매년 많은 종류의 물고기가 멸종되고 있다.\\n외래종의 유입으로 사라지는 토착종: 외래종의 유입은 자연스럽게 발생할 수 있으나, 인위적인 목적으로 이루어지면 심각한 위험을 불러올 수 있다.\\n무분별하게 도입해 방사한 황소개구리에 의해 여러 종류의 민물고기가 멸종 위기에 처해 있는 상황과 나일 농어의 유입으로 200여 종의 물고기가 사라진 아프리카 빅토리아호가 대표적인 사례이다.\\n수질 오염에 따른 수상 생물 종의 감소: 농업 생산량을 늘리고자 사용하는 질소와 인산 성분 비료는 장기적으로 토양뿐만 아니라 하천과 바다를 오염시킨다.\\n1860년 이후 바다로 흘러들어 간 질소의 양은 2배로 늘어났다.\\n바다에 녹아든 질소는 조류 (藻類)의 이상 번식을 일으켜 산소를 고갈시키고 수상 생물을 떼죽음으로 내몬다.\"\n",
        "    },\n",
        "    {\n",
        "        \"교과목\": \"사회\",\n",
        "        \"텍스트\": \"인간 개발 지수 (HDI): 각국의 교육 수준과 국민 소득, 기대 수명 등을 기준으로 발전 정도를 평가하는 지수이다.\"\n",
        "    },\n",
        "    {\n",
        "        \"교과목\": \"사회\",\n",
        "        \"텍스트\": \"세계 각 지역은 부존 자원 정도, 산업화 시기, 역사적 배경 등의 영향으로 발전 수준에 차이가 난다.\\n일반적으로 저개발 지역은 발전 지역보다 소득 수준이 낮으며 학교나 의료 시설 등이 부족하고 주거 환경도 열악하여 삶의 질이 낮다.\\n저개발 지역들은 빈곤에 따른 여러 사회 문제를 해결하고 경제 발전을 이루어 삶의 질을 높이고자 적극적으로 노력하고 있다.\\n사회 기반 시설을 확충하기 위해 외국 자본의 유치를 적극적으로 추진하고 있으며, 선진 기술의 도입으로 산업 부분에서의 생산성을 높이고 있다.\\n최근에는 세계화에 따라 국가 사이의 경쟁이 심해지면서 이에 대응하려는 저개발 지역들이 경제 협력 체제를 만들고 있다.\\n실제로 2009 ~ 2013년 동안 저개발 지역 국가들의 연평균 경제 성장률은 선진국들보다 높게 나타난다.\"\n",
        "    },\n",
        "    {\n",
        "        \"교과목\": \"사회\",\n",
        "        \"텍스트\": \"경제 협력 체제는 여러 국가가 세계화에 대응하고 경쟁력을 높이고자 구성하는 것이다.\\n특히, 단일 국가의 능력으로 선진국들의 자본, 기술과 경쟁하기 어려운 저개발 국가들은 협력 체제를 구축하여 공동으로 자원을 개발하여 수출하고, 자국의 이익에 부정적인 영향을 미치는 국가들에 함께 대응하는 노력이 이루어지고 있다.\\n경제 협력 체제 회원국 간에는 수출 시 부과하는 세금을 낮추거나 없애 교류를 확대하고 이를 바탕으로 상호 보완적인 경제 발전을 추진할 수 있다.\"\n",
        "    },\n",
        "    {\n",
        "        \"교과목\": \"사회\",\n",
        "        \"텍스트\": \"국제 사회는 세계 각 지역의 불평등을 완화하고자 다양한 방면으로 노력하고 있다.\\n국제 연합을 중심으로 각국 정부는 공적 개발 원조를 통한 경제적 지원뿐만 아니라 스포츠, 문화 등 다양한 분야에서 협력하고 있다.\\n민간 및 개인 차원에서의 협력도 활발히 이루어지고 있다.\\n특히 비정부 기구 (NGO)는 저개발 지역의 어려운 현실을 시민들에게 알리고 이들 지역 주민들을 돕는 다양한 기회를 제공한다.\"\n",
        "    },\n",
        "    {\n",
        "        \"교과목\": \"사회\",\n",
        "        \"텍스트\": \"공적 개발 원조: 선진국의 정부 또는 공공 기관이 저개발국의 경제 사회 발전과 복지 증진을 주목적으로 자금이나 기술을 지원하는 제도이다.\"\n",
        "    },\n",
        "    {\n",
        "        \"교과목\": \"사회\",\n",
        "        \"텍스트\": \"비정부 기구 (Non-Goverment Organization): 권력이나 이윤을 추구하지 않고 조직된 자발적인 시민 단체이다.\"\n",
        "    },\n",
        "]\n",
        "texts=[]\n",
        "for i in range(len(data)):\n",
        "  texts.append(data[i][\"텍스트\"].replace(\"\\n\",\"\"))\n",
        "print(texts)  \n"
      ],
      "metadata": {
        "id": "ha3GVviW1ukT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kor_tokenized_doc = [text.split() for text in texts]\n",
        "print(len(kor_tokenized_doc))"
      ],
      "metadata": {
        "id": "V_AtcgrMLFDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "jFwrG58XLQSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(kor_tokenized_doc)"
      ],
      "metadata": {
        "id": "3YDfMMRyLRqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = tokenizer.word_index\n",
        "idx2word = {value : key for key, value in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(kor_tokenized_doc)"
      ],
      "metadata": {
        "id": "gOHiWK_KLTCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2idx) + 1 \n",
        "print(f\"단어 사전의 크기 : {vocab_size}\")"
      ],
      "metadata": {
        "id": "6bWcWIATLWc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded[0])"
      ],
      "metadata": {
        "id": "MesJaVDRLYvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import skipgrams"
      ],
      "metadata": {
        "id": "spHNFopqLc6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 네거티브 샘플링\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:5]]\n",
        "print(f\"전체 샘플 수 : {len(skip_grams)}\")"
      ],
      "metadata": {
        "id": "76XNKRP6Lgvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]"
      ],
      "metadata": {
        "id": "Dd-paOIALjhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
        "print(len(pairs))\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "Lt0QTHrFLmNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pairs)"
      ],
      "metadata": {
        "id": "cSZrjmlSMBjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "    idx2word[pairs[i][0]], pairs[i][0], \n",
        "    idx2word[pairs[i][1]], pairs[i][1], \n",
        "    labels[i])\n",
        "  )"
      ],
      "metadata": {
        "id": "t4zMiCsvLn-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
      ],
      "metadata": {
        "id": "HMhE0Sg0Lqh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG"
      ],
      "metadata": {
        "id": "QahYAnU8LzY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "# 중심 단어를 위한 임베딩 테이블\n",
        "w_inputs = Input(shape=(1, ), dtype='int32')\n",
        "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n",
        "\n",
        "# 주변 단어를 위한 임베딩 테이블\n",
        "c_inputs = Input(shape=(1, ), dtype='int32')\n",
        "context_embedding  = Embedding(vocab_size, embedding_dim)(c_inputs)"
      ],
      "metadata": {
        "id": "0ITgG5JXL2uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "output = Activation('sigmoid')(dot_product)\n",
        "\n",
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
      ],
      "metadata": {
        "id": "AM4CAygPL54e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  loss = 0\n",
        "  for _, elem in enumerate(skip_grams):\n",
        "    first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "    second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "    labels = np.array(elem[1], dtype='int32')\n",
        "    X = [first_elem, second_elem]\n",
        "    Y = labels\n",
        "    loss += model.train_on_batch(X,Y)  \n",
        "  print('Epoch :',epoch + 1, 'Loss :',loss)"
      ],
      "metadata": {
        "id": "LZ5GBuoTMdEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('kor_vectors.txt' ,'w')\n",
        "f.write('{} {}\\n'.format(vocab_size-1, embedding_dim))\n",
        "vectors = model.get_weights()[0]\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "N243nejNMhza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 로드\n",
        "import re\n",
        "w2v = gensim.models.KeyedVectors.load_word2vec_format('./kor_vectors.txt', binary=False)\n",
        "for i in range(len(texts)):\n",
        "  # print(texts[i])\n",
        "  cur_text=texts[i]\n",
        "  \n",
        "  # print(type(cur_text))\n",
        "  \n",
        "  cur_list=re.split(\" |\\.\",cur_text)\n",
        "  # print(re.split(\" |\\.\",cur_text))\n",
        "  # for j in range(len(cur_list)):\n",
        "  #   cur_word=cur_list[j]\n",
        "  #   if cur_word in w2v:\n",
        "  #     k=w2v.most_similar(negative=[cur_word])\n",
        "  #     print(k)\n",
        "\n",
        "# if \"선진국\" in w2v:\n",
        "#   print(\"hello\")"
      ],
      "metadata": {
        "id": "0vv0ey-pMl97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=w2v.most_similar(negative=['선진국'])\n",
        "print(k)\n",
        "# print(t)\n",
        "# for i in len(texts):\n"
      ],
      "metadata": {
        "id": "GYKdPaD0MneS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ndUE_qELMsdv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}